{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "import keras\n",
    "max_features = 1000\n",
    "\n",
    "max_length = 50\n",
    "(x_train,y_train),(x_test, y_test) = imdb.load_data(num_words = max_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,  56,  26, ...,  19, 178,  32],\n",
       "       [  2,   5,   2, ...,  16, 145,  95],\n",
       "       [215,  28, 610, ...,   7, 129, 113],\n",
       "       ...,\n",
       "       [  4,  65, 496, ...,   4,   2,   2],\n",
       "       [ 13,  18,  31, ...,  12,   9,  23],\n",
       "       [  2,   8,   2, ..., 204, 131,   9]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train= preprocessing.sequence.pad_sequences(x_train, maxlen = max_length)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test= preprocessing.sequence.pad_sequences(x_test, maxlen = max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 802       \n",
      "=================================================================\n",
      "Total params: 80,802\n",
      "Trainable params: 80,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000,8,input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['acc'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and testing a model on imdb review data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s 58us/step - loss: 0.6261 - acc: 0.6596 - val_loss: 0.5180 - val_acc: 0.7576\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s 53us/step - loss: 0.4598 - acc: 0.7843 - val_loss: 0.4532 - val_acc: 0.7796\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 0.4179 - acc: 0.8074 - val_loss: 0.4445 - val_acc: 0.7828\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 0.4017 - acc: 0.8145 - val_loss: 0.4451 - val_acc: 0.7896\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 0.3887 - acc: 0.8239 - val_loss: 0.4476 - val_acc: 0.7914\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 0.3769 - acc: 0.8306 - val_loss: 0.4522 - val_acc: 0.7886\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 0.3649 - acc: 0.8371 - val_loss: 0.4592 - val_acc: 0.7868\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s 47us/step - loss: 0.3535 - acc: 0.8432 - val_loss: 0.4644 - val_acc: 0.7850\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s 48us/step - loss: 0.3422 - acc: 0.8502 - val_loss: 0.4731 - val_acc: 0.7782\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s 54us/step - loss: 0.3305 - acc: 0.8573 - val_loss: 0.4818 - val_acc: 0.7798\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 0s 12us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4756104110717773, 0.77988]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**trying to tokenize our transcripts and building a basic model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = pd.read_csv(\"C:/University of Chicago/Project/MOUD/TranslatedTransripts/AllText.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TranslatedText</th>\n",
       "      <th>Annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I had seen remarks that said it stings when yo...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>and the truth is that if I use it once and t a...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>and I said no it could be possible so much I w...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>This also pulls a little hair but do it as it ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>but the same with the washings has stopped bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>em they wash super easy they dry fast they dry...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>and good with the washings no longer scrape so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>it is already bearable</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>well yes it stings a p</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>And what I love about this movie is that it39s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                     TranslatedText  Annotation\n",
       "0           1  I had seen remarks that said it stings when yo...          -1\n",
       "1           2  and the truth is that if I use it once and t a...          -1\n",
       "2           3  and I said no it could be possible so much I w...          -1\n",
       "3           4  This also pulls a little hair but do it as it ...          -1\n",
       "4           5  but the same with the washings has stopped bei...           1\n",
       "5           6  em they wash super easy they dry fast they dry...           1\n",
       "6           7  and good with the washings no longer scrape so...           1\n",
       "7           8                             it is already bearable           1\n",
       "8           9                             well yes it stings a p          -1\n",
       "9          10  And what I love about this movie is that it39s...           1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts = transcripts[transcripts.Annotation !=0]\n",
    "transcripts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = transcripts[\"Annotation\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = transcripts[\"TranslatedText\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)==len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1 if x==1 else 0 for x in labels] #converting to format used in the Chollet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokeinizing the text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=50\n",
    "training_samples=250\n",
    "validation_samples = 200\n",
    "max_words = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= Tokenizer(num_words= max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1241 unique tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"found {} unique tokens\".format(len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data tensor: (450, 50)\n",
      "shape of labels tensor (450, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of data tensor:\",data.shape)\n",
    "print(\"shape of labels tensor\",labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2 = data[:training_samples]\n",
    "y_train2 = labels[:training_samples]\n",
    "x_test2= data[training_samples:training_samples+validation_samples]\n",
    "y_test2 = labels[training_samples:training_samples+validation_samples]\n",
    "# convert class vectors to binary class matrices\n",
    "y_train2 = keras.utils.to_categorical(y_train2, num_classes)\n",
    "y_test2 = keras.utils.to_categorical(y_test2, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=2\n",
    "labels = keras.utils.to_categorical(labels, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 0s 35us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.051520997153388, 0.5622222222222222]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**parsing the glove word embeddings file to use pre-trained glove embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "glove_dir = \"C:/University of Chicago/Project/glove.6B/glove.6B.50d.txt\"\n",
    "embeddings_index={}\n",
    "f=open(glove_dir,encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word=values[0]\n",
    "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "    embeddings_index[word]=coefs\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the Glove word embeddings matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_vector = ()\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the model on imdb data with pretrained glove embeddings and Dense layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 50)            500000    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2500)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                80032     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 580,098\n",
      "Trainable params: 580,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model definition\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(max_words,embedding_dim,input_length=maxlen))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(32,activation='relu'))\n",
    "model1.add(Dense(2,activation='softmax'))\n",
    "model1.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['acc'])\n",
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.layers[0].set_weights([embedding_matrix])\n",
    "model1.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 50 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 0s 993us/step - loss: 0.7971 - acc: 0.4800 - val_loss: 0.7786 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 0s 124us/step - loss: 0.5189 - acc: 0.7700 - val_loss: 0.7413 - val_acc: 0.5800\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 0s 219us/step - loss: 0.3883 - acc: 0.8600 - val_loss: 0.8091 - val_acc: 0.5600\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 0s 156us/step - loss: 0.3206 - acc: 0.9100 - val_loss: 0.6972 - val_acc: 0.6000\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 0s 152us/step - loss: 0.2449 - acc: 0.9700 - val_loss: 0.7521 - val_acc: 0.6000\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 0s 131us/step - loss: 0.2190 - acc: 0.9600 - val_loss: 0.7787 - val_acc: 0.5800\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 0s 173us/step - loss: 0.1711 - acc: 0.9900 - val_loss: 0.8157 - val_acc: 0.6400\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 0s 158us/step - loss: 0.1350 - acc: 1.0000 - val_loss: 0.6985 - val_acc: 0.6000\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 0s 134us/step - loss: 0.1096 - acc: 0.9900 - val_loss: 0.8513 - val_acc: 0.6000\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 0s 172us/step - loss: 0.0971 - acc: 1.0000 - val_loss: 0.7585 - val_acc: 0.5200\n"
     ]
    }
   ],
   "source": [
    "model1.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['acc'])\n",
    "history = model1.fit(x_train, y_train, epochs=10, batch_size=32,validation_split=0.2)\n",
    "model1.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 66us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7905377435684204, 0.675]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.load_weights('pre_trained_glove_model.h5')\n",
    "model1.evaluate(x_train2,y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the same model without pretrained embeddings and Dense layers**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 50, 50)            500000    \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2500)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                80032     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 580,098\n",
      "Trainable params: 580,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(max_words,embedding_dim,input_length=maxlen))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(32,activation='relu'))\n",
    "model2.add(Dense(2,activation='softmax'))\n",
    "model2.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['acc'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 50 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6988 - acc: 0.4950 - val_loss: 0.7180 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 0s 723us/step - loss: 0.6624 - acc: 0.5900 - val_loss: 0.6984 - val_acc: 0.5200\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 0s 528us/step - loss: 0.6291 - acc: 0.6700 - val_loss: 0.6996 - val_acc: 0.5200\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 0s 637us/step - loss: 0.5857 - acc: 0.7250 - val_loss: 0.6764 - val_acc: 0.5800\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 0s 522us/step - loss: 0.5099 - acc: 0.9150 - val_loss: 0.7116 - val_acc: 0.5200\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 0s 451us/step - loss: 0.4372 - acc: 0.8950 - val_loss: 0.6650 - val_acc: 0.5600\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 0s 490us/step - loss: 0.3430 - acc: 0.9650 - val_loss: 0.6612 - val_acc: 0.6000\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 0s 416us/step - loss: 0.2607 - acc: 0.9950 - val_loss: 0.6538 - val_acc: 0.6000\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 0s 426us/step - loss: 0.1986 - acc: 0.9850 - val_loss: 0.6382 - val_acc: 0.5400\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 0s 305us/step - loss: 0.1435 - acc: 1.0000 - val_loss: 0.6645 - val_acc: 0.6200\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['acc'])\n",
    "history = model2.fit(x_train, y_train, epochs=10, batch_size=32,validation_split=0.2)\n",
    "model2.save_weights('densenonpretrained')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 0s 54us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0038612450493707, 0.5355555555555556]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(data,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training a model with imdb data, RNN layers **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 50, 50)            500000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_13 (SimpleRNN)    (None, 50, 32)            2656      \n",
      "_________________________________________________________________\n",
      "simple_rnn_14 (SimpleRNN)    (None, 50, 32)            2080      \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                51232     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 556,034\n",
      "Trainable params: 556,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(max_words,embedding_dim,input_length=maxlen))\n",
    "model3.add(SimpleRNN(32, return_sequences=True))\n",
    "model3.add(SimpleRNN(32, return_sequences=True))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(32, activation=\"relu\"))\n",
    "model3.add(Dense(2, activation =\"softmax\"))\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 16s 781us/step - loss: 0.6969 - acc: 0.4985 - val_loss: 0.6931 - val_acc: 0.5062\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 14s 723us/step - loss: 0.6932 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5062\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 16s 775us/step - loss: 0.6932 - acc: 0.4995 - val_loss: 0.6932 - val_acc: 0.4938\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 16s 802us/step - loss: 0.6932 - acc: 0.4973 - val_loss: 0.6931 - val_acc: 0.5062\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 17s 858us/step - loss: 0.6932 - acc: 0.5019 - val_loss: 0.6932 - val_acc: 0.4938\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 17s 852us/step - loss: 0.6932 - acc: 0.5005 - val_loss: 0.6931 - val_acc: 0.5062\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 17s 866us/step - loss: 0.6932 - acc: 0.4977 - val_loss: 0.6932 - val_acc: 0.4938\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 16s 820us/step - loss: 0.6932 - acc: 0.4995 - val_loss: 0.6933 - val_acc: 0.4938\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 18s 879us/step - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6934 - val_acc: 0.4938\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 17s 861us/step - loss: 0.6932 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5062a\n"
     ]
    }
   ],
   "source": [
    "history = model3.fit(x_train, y_train, epochs=10, batch_size=32,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model3.save_weights('simpleRNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 0s 245us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6929802269405789, 0.5511111111111111]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(data,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training the model with imdb data, glove embedding and simple LSTM layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "##training the model with embedding and simple LSTM layers\n",
    "from keras.layers import LSTM \n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 50, 50)            500000    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 50, 32)            10624     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 50, 32)            8320      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 50, 32)            8320      \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 32)                51232     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 578,562\n",
      "Trainable params: 578,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Embedding(max_words,embedding_dim,input_length=maxlen))\n",
    "model4.add(LSTM(32, return_sequences=True))\n",
    "model4.add(Dropout(0.5))\n",
    "model4.add(LSTM(32, return_sequences=True))\n",
    "model4.add(Dropout(0.25))\n",
    "model4.add(LSTM(32, return_sequences=True))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(32, activation=\"relu\"))\n",
    "model4.add(Dropout(0.25))\n",
    "model4.add(Dense(2, activation =\"softmax\"))\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.layers[0].set_weights([embedding_matrix])\n",
    "model4.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 50s 2ms/step - loss: 0.6930 - acc: 0.5194 - val_loss: 0.6942 - val_acc: 0.5376\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 51s 3ms/step - loss: 0.6792 - acc: 0.5685 - val_loss: 0.6902 - val_acc: 0.5488\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.6484 - acc: 0.6142 - val_loss: 0.6318 - val_acc: 0.6246\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 50s 3ms/step - loss: 0.6243 - acc: 0.6411 - val_loss: 0.6230 - val_acc: 0.6362\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 46s 2ms/step - loss: 0.6023 - acc: 0.6707 - val_loss: 0.6033 - val_acc: 0.6548\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 46s 2ms/step - loss: 0.5826 - acc: 0.6845 - val_loss: 0.5951 - val_acc: 0.6622\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.5644 - acc: 0.7025 - val_loss: 0.5604 - val_acc: 0.6988\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 49s 2ms/step - loss: 0.5438 - acc: 0.7138 - val_loss: 0.5402 - val_acc: 0.7140\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 49s 2ms/step - loss: 0.5319 - acc: 0.7258 - val_loss: 0.5751 - val_acc: 0.6918\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 44s 2ms/step - loss: 0.5152 - acc: 0.7373 - val_loss: 0.5842 - val_acc: 0.7088\n"
     ]
    }
   ],
   "source": [
    "history = model4.fit(x_train, y_train, epochs=10, batch_size=32,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.save_weights('LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 0s 609us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9248037634955513, 0.47555555555555556]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.evaluate(data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
